{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet for Dosimetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### UNet with 1 channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Update the notebook to incorporate any changes made in the environment\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    " %reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# training and test modules import \n",
    "from src.test import test_loop\n",
    "from src.train import train_loop\n",
    "from src.eval import eval\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Unet\n",
    "from monai.networks.nets import UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.load_XY import load_XY_1chanel\n",
    "\n",
    "# Dataset import:\n",
    "data_folder = \"./data\"\n",
    "\n",
    "X, y = load_XY_1chanel(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6560, 6560)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for length\n",
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Separating the data into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Combine X and y into a list of tuples\n",
    "data = list(zip(X, y))\n",
    "\n",
    "# Assuming each patient has 80 lists of tensors\n",
    "lists_per_patient = 80\n",
    "\n",
    "# Total number of patients\n",
    "total_patients = len(data) // lists_per_patient\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Shuffle the indices of patients\n",
    "shuffled_patients = list(range(total_patients))\n",
    "random.shuffle(shuffled_patients)\n",
    "\n",
    "# Calculate the number of patients for each split\n",
    "train_patients = 58\n",
    "val_patients = 16\n",
    "test_patients = 8\n",
    "\n",
    "# Extract data for each split based on the shuffled indices\n",
    "train_indices = shuffled_patients[:train_patients]\n",
    "val_indices = shuffled_patients[train_patients: train_patients + val_patients]\n",
    "test_indices = shuffled_patients[train_patients + val_patients: train_patients + val_patients + test_patients]\n",
    "\n",
    "# Extract data for each split\n",
    "train_data = [data[i * lists_per_patient: (i + 1) * lists_per_patient] for i in train_indices]\n",
    "val_data = [data[i * lists_per_patient: (i + 1) * lists_per_patient] for i in val_indices]\n",
    "test_data = [data[i * lists_per_patient: (i + 1) * lists_per_patient] for i in test_indices]\n",
    "\n",
    "# Flatten the lists of lists into a single list for X and y\n",
    "X_train, y_train = zip(*[item for sublist in train_data for item in sublist])\n",
    "X_val, y_val = zip(*[item for sublist in val_data for item in sublist])\n",
    "X_test, y_test = zip(*[item for sublist in test_data for item in sublist])\n",
    "\n",
    "# Convert train, test and val into numpy array\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of training set: (4640, 4640)\n",
      "\n",
      "Dimension of test set: (640, 640)\n",
      "\n",
      "Dimension of validation set: (1280, 1280)\n"
     ]
    }
   ],
   "source": [
    "# lenghts of train, test and val\n",
    "print(f\"Dimension of training set: {len(X_train), len(y_train)}\\n\")\n",
    "print(f\"Dimension of test set: {len(X_test), len(y_test)}\\n\")\n",
    "print(f\"Dimension of validation set: {len(X_val), len(y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert numpy array to tensors\n",
    "X_train = torch.from_numpy(X_train).to(dtype=torch.float32)\n",
    "X_val = torch.from_numpy(X_val).to(dtype=torch.float32)\n",
    "X_test = torch.from_numpy(X_test).to(dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Match y dimension to the X\n",
    "# y_train is a NumPy array\n",
    "y_train = y_train[:, np.newaxis, :, :]\n",
    "# Now, y_train has shape (4640, 1, 64, 64)\n",
    "\n",
    "y_val = y_val[:, np.newaxis, :, :]\n",
    "y_test = y_test[:, np.newaxis, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4640, 1, 64, 64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape of y\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "# Creation of dataloaders\n",
    "train_dataloader = DataLoader(CustomDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "validation_dataloader = DataLoader(CustomDataset(X_val, y_val), batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(CustomDataset(X_test, y_test), batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (e11): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (e12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (e21): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (e22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (e31): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (e32): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (e41): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (e42): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (e51): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (e52): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (upconv1): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (d11): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (d12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (upconv2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (d21): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (d22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (d31): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (d32): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (upconv4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (d41): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (d42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (outconv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from src.unet_from_scratch import UNet\n",
    "import torch\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the model\n",
    "model = UNet(n_class=1, in_channels=1).to(device)\n",
    "\n",
    "# print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "model_save_path = \"./model/unet_1c_100ep_MAE.pth\"\n",
    "loss_fn = nn.L1Loss()  \n",
    "epochs = 100\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "eval(model, train_dataloader, validation_dataloader, test_dataloader, loss_fn, optimizer, epochs, device, model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.predict import predict1\n",
    "from src.visualization import visualize_prediction_1c\n",
    "import torch\n",
    "\n",
    "# load the images for visualization\n",
    "low = \"./test/low_edep.mhd\"\n",
    "ct = \"./test/ct.mhd\"\n",
    "ground_truth_data = \"./test/high_edep.mhd\"\n",
    "annotated = \"./test/annotated_ct.npy\"\n",
    "\n",
    "# define paths for loading the features and saving the output visualization\n",
    "paths = \"./test/low_edep.mhd\"\n",
    "\n",
    "\n",
    "# define path for model trained\n",
    "model_path = './model/unet_1c_100ep_MAE.pth'\n",
    "\n",
    "\n",
    "# load the model\n",
    "model = torch.load(model_path)\n",
    "\n",
    "\n",
    "# make prediction\n",
    "predicted_data = predict1(model, paths, device=device)\n",
    "\n",
    "# Visualisation\n",
    "visualize_prediction_1c(low, ground_truth_data, predicted_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
